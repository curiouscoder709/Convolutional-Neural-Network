{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77271820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import pattern\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import contractions\n",
    "from collections import  Counter\n",
    "import emot\n",
    "import re\n",
    "import pickle\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Query Search\n",
    "base_url=\"https://www.amazon.in/s?k=\"\n",
    "search_query=\"sports+shoes&crid=3D9RT0WJS6QZC&sprefix=sports+shoes%2Caps%2C287&ref=nb_sb_noss_1\"\n",
    "url=base_url+search_query\n",
    "\n",
    "# Setting header for the scraper\n",
    "header={\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko)Chrome/77.0.3865.90 Safari/537.36'}\n",
    "\n",
    "# Getting the html data\n",
    "search_response=requests.get(url,headers=header)\n",
    "\n",
    "# Checking response status\n",
    "search_response.status_code\n",
    "\n",
    "# Getting the html data\n",
    "search_response.text\n",
    "\n",
    "# Using BeautifulSoup to parse the data\n",
    "soup = BeautifulSoup(search_response.content)\n",
    "data_asin = []\n",
    "\n",
    "# Finding ASIN product codes\n",
    "for i in soup.find_all('div',\n",
    "                    class_= 'sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20'):\n",
    "    data_asin.append(i[\"data-asin\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search ASIN\n",
    "def Searchasin(asin):\n",
    "    url=\"https://www.amazon.in/dp/\"+asin\n",
    "    print(url)\n",
    "    page=requests.get(url,headers=header)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5631f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list to save the links of the pages where the reviews are\n",
    "link=[]\n",
    "for i in range(len(data_asin)):\n",
    "    response=Searchasin(data_asin[i])\n",
    "    soup=BeautifulSoup(response.content)\n",
    "    for i in soup.findAll(\"a\",{'data-hook':\"see-all-reviews-link-foot\"}):\n",
    "        link.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b58f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search reviews using ASIN\n",
    "def Searchreviews(review_link):\n",
    "    url=\"https://www.amazon.in\"+review_link\n",
    "    print(url)\n",
    "    page=requests.get(url,headers=header)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 4 lists to scrape the required data\n",
    "reviews=[]\n",
    "star=[]\n",
    "title=[]\n",
    "name=[]\n",
    "for j in range(len(link)):\n",
    "    for k in range(100):\n",
    "        response=Searchreviews(link[j]+'&pageNumber='+str(k))\n",
    "        \n",
    "        soup=BeautifulSoup(response.content)\n",
    "        for i in soup.findAll(\"span\",{'data-hook':\"review-body\"}):\n",
    "            reviews.append(i.text)\n",
    "        for i in soup.findAll(\"i\",{'data-hook':'review-star-rating'}):\n",
    "            star.append(i.text)\n",
    "        for i in soup.findAll(\"a\",{'data-hook':'review-title'}):\n",
    "            title.append(i.text)\n",
    "        for i in soup.findAll(\"span\",{'data-hook':\"review-body\"}):\n",
    "            name.append(\"https://www.amazon.in\"+link[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe and exporting as a csv file for analysis\n",
    "df1 = pd.DataFrame({'name':name,'title':title,'reviews':reviews,'star':star})\n",
    "df1.to_csv(\"Amazon_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6341f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data\n",
    "data = pd.read_csv('C:/Users/malay/Amazon reviews.csv')\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465da985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of data\n",
    "df = data.copy()\n",
    "\n",
    "# Checking for null values and dropping them\n",
    "df.isnull().sum()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting reviews to string datatype\n",
    "df['reviews'] = df['reviews'].astype(str)\n",
    "\n",
    "# Dropping Duplicates\n",
    "df = df.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Removing '\\n' present in reviews\n",
    "df['reviews']=df['reviews'].apply(lambda x:x.strip('\\n'))\n",
    "df['title']=df['title'].apply(lambda x:x.strip('\\n'))\n",
    "\n",
    "# Removing strings that are not required\n",
    "df['fin_review'] = df['fin_review'].apply(lambda x : x.replace(\"The media could not be loaded\",\"\"))\n",
    "df['fin_review'] = df['fin_review'].apply(lambda x : x.replace(\"nan\",\"\"))\n",
    "\n",
    "# Merging title and content of reviews\n",
    "df['fin_review'] = df['title'] + ' ' + df['reviews']\n",
    "\n",
    "# Getting the name of the brand from URL\n",
    "df['name'] = df['name'].apply(lambda x : x.replace(\"https://www.amazon.in/\",\"\"))\n",
    "df['name'] = df['name'].apply(lambda x:x.partition('-')[0])\n",
    "\n",
    "# Getting the star value\n",
    "df['star'] = df['star'].apply(lambda x:x[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emojis into word\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "df['fin_review'] = df['fin_review'].apply(lambda x : convert_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638892f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get POS Tagging\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17192d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the data(stopwords, punctuations,lowercase,contractions)\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in STOPWORDS]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "df['clean_fin_review'] = df['fin_review'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41078dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the VADER module\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment score\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "# Function to calculate compound score\n",
    "def compound_score(text):\n",
    "    comp=sentiment_analyzer_scores(text)\n",
    "    return comp['compound'] # returns the compound score from the dictionary\n",
    "\n",
    "# Applying on the reviews column to get the score\n",
    "df['sentiment_score']=df['clean_fin_review'].apply(lambda x:compound_score(x)) \n",
    "\n",
    "# Function to classify category for reviews\n",
    "def sentiment_category(score):\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df['review_category']=df['sentiment_score'].apply(lambda x:sentiment_category(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return words with the highest frequencies\n",
    "def getMostCommon(reviews_list,topn=20):\n",
    "    reviews=\" \".join(reviews_list)\n",
    "    tokenised_reviews=reviews.split(\" \")\n",
    "    \n",
    "    \n",
    "    freq_counter=Counter(tokenised_reviews)\n",
    "    return freq_counter.most_common(topn) \n",
    "\n",
    "# Function to plot the most common top words in reviews\n",
    "def plotMostCommonWords(reviews_list,topn=20,title=\"Common Review Words\",color=\"mediumseagreen\"): \n",
    "    top_words=getMostCommon(reviews_list,topn=topn)\n",
    "    data=pd.DataFrame()\n",
    "    data['words']=[val[0] for val in top_words]\n",
    "    data['freq']=[val[1] for val in top_words]\n",
    "    ax = sns.barplot(y='words',x='freq',data=data,color=color).set_title(title+\" top \"+str(topn))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# Function to generate n-grams\n",
    "def generateNGram(text,n):\n",
    "    tokens=text.split(\" \")\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\"_\".join(ngram) for ngram in ngrams]\n",
    "reviews_bigrams=[\" \".join(generateNGram(i,2)) for i in df['clean_fin_review']]\n",
    "plotMostCommonWords(reviews_bigrams,20,\"Review Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7828cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to csv for further analysis in PowerBi\n",
    "df.to_csv(\"Amazon Reviews Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data for Twitter Analysis \n",
    "data = pd.read_csv('C:/Users/malay/Downloads/archive (7)/twitter_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of data\n",
    "df = data.copy()\n",
    "\n",
    "# Drop irrelevant and neutral sentiment\n",
    "df.drop(df[df['Sentiment'] == 'Irrelevant'].index, inplace = True)\n",
    "df.drop(df[df['Sentiment'] == 'Neutral'].index, inplace = True)\n",
    "\n",
    "# Drop Duplicates\n",
    "df1 = df.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Drop Missing values\n",
    "df2 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tweets to string\n",
    "df2['tweets'] = df2['tweets'].astype(str)\n",
    "\n",
    "# Add a length of tweets column\n",
    "df2['tweet_length'] = df2['tweets'].str.len()\n",
    "\n",
    "# Remove HTML tags\n",
    "df2['tweets']=df2['tweets'].apply(lambda x: BeautifulSoup(x,'lxml').get_text())\n",
    "\n",
    "# Remove URL's\n",
    "df2['tweets']=df2['tweets'].apply(lambda x: re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+\n",
    "                                    [.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+\n",
    "                                    (?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\",x))\n",
    "\n",
    "# Remove E-mails\n",
    "df2['tweets']=df2['tweets'].apply(lambda x: re.sub(r'([a-zA-Z0-9+_-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove accentuated characters\n",
    "def remove_acc_chars(x):\n",
    "    x=unicodedata.normalize('NFKD',x).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return x\n",
    "df2['tweets']=df2['tweets'].apply(lambda x: remove_acc_chars(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the clean text function from above \n",
    "df2['clean_tweets'] = df2['tweets'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to csv for further analysis in PowerBi \n",
    "df2.to_csv(\"Twitter Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17186843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
